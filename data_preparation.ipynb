{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be used to prepare the data for machine learning.\n",
    "\n",
    "1. Annotate the dataset (Sleep 0 /Awake 1)\n",
    "2. Signal Preparation (scaling, missing data, outliers, smoothing)\n",
    "3. Subset generation (light, medium, heavy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import polars as pl\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert timestamp to datetime**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = [\n",
    "    pl.col(\"timestamp\").str.to_datetime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert angle to radians**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deg_to_rad(x: pl.Expr) -> pl.Expr:\n",
    "    return np.pi / 180 * x\n",
    "\n",
    "angleConversion = [\n",
    "    deg_to_rad(pl.col(\"anglez\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Min-max normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_normalization = lambda x: (x - x.min()) / (x.max() - x.min())\n",
    "normalization = [\n",
    "    pl.col(\"anglez\").map_batches(min_max_normalization).cast(pl.Float32), \n",
    "    pl.col(\"enmo\").map_batches(min_max_normalization).cast(pl.Float32),\n",
    "    pl.col(\"step\").cast(pl.UInt32),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_signals = pl.scan_parquet(\"data/test_series.parquet\").with_columns(\n",
    "    timestamp + angleConversion + normalization\n",
    ").collect(streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ComputeError",
     "evalue": "error open file: data/test_events.csv, error open file: data/test_events.csv, No such file or directory (os error 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mComputeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/olivier/Documents/projet/DetectSleepStates/data_preparation.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/olivier/Documents/projet/DetectSleepStates/data_preparation.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_events \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39;49mscan_csv(\u001b[39m\"\u001b[39;49m\u001b[39mdata/test_events.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mwith_columns(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/olivier/Documents/projet/DetectSleepStates/data_preparation.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     timestamp \u001b[39m+\u001b[39m [pl\u001b[39m.\u001b[39mcol(\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcast(pl\u001b[39m.\u001b[39mUInt32)]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/olivier/Documents/projet/DetectSleepStates/data_preparation.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m )\u001b[39m.\u001b[39mdrop_nulls()\u001b[39m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.10/site-packages/polars/io/csv/functions.py:911\u001b[0m, in \u001b[0;36mscan_csv\u001b[0;34m(source, has_header, separator, comment_char, quote_char, skip_rows, dtypes, schema, null_values, missing_utf8_is_empty_string, ignore_errors, cache, with_column_names, infer_schema_length, n_rows, encoding, low_memory, rechunk, skip_rows_after_header, row_count_name, row_count_offset, try_parse_dates, eol_char, new_columns, raise_if_empty, truncate_ragged_lines)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    909\u001b[0m     source \u001b[39m=\u001b[39m [normalize_filepath(source) \u001b[39mfor\u001b[39;00m source \u001b[39min\u001b[39;00m source]\n\u001b[0;32m--> 911\u001b[0m \u001b[39mreturn\u001b[39;00m pl\u001b[39m.\u001b[39;49mLazyFrame\u001b[39m.\u001b[39;49m_scan_csv(\n\u001b[1;32m    912\u001b[0m     source,\n\u001b[1;32m    913\u001b[0m     has_header\u001b[39m=\u001b[39;49mhas_header,\n\u001b[1;32m    914\u001b[0m     separator\u001b[39m=\u001b[39;49mseparator,\n\u001b[1;32m    915\u001b[0m     comment_char\u001b[39m=\u001b[39;49mcomment_char,\n\u001b[1;32m    916\u001b[0m     quote_char\u001b[39m=\u001b[39;49mquote_char,\n\u001b[1;32m    917\u001b[0m     skip_rows\u001b[39m=\u001b[39;49mskip_rows,\n\u001b[1;32m    918\u001b[0m     dtypes\u001b[39m=\u001b[39;49mdtypes,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    919\u001b[0m     schema\u001b[39m=\u001b[39;49mschema,\n\u001b[1;32m    920\u001b[0m     null_values\u001b[39m=\u001b[39;49mnull_values,\n\u001b[1;32m    921\u001b[0m     missing_utf8_is_empty_string\u001b[39m=\u001b[39;49mmissing_utf8_is_empty_string,\n\u001b[1;32m    922\u001b[0m     ignore_errors\u001b[39m=\u001b[39;49mignore_errors,\n\u001b[1;32m    923\u001b[0m     cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m    924\u001b[0m     with_column_names\u001b[39m=\u001b[39;49mwith_column_names,\n\u001b[1;32m    925\u001b[0m     infer_schema_length\u001b[39m=\u001b[39;49minfer_schema_length,\n\u001b[1;32m    926\u001b[0m     n_rows\u001b[39m=\u001b[39;49mn_rows,\n\u001b[1;32m    927\u001b[0m     low_memory\u001b[39m=\u001b[39;49mlow_memory,\n\u001b[1;32m    928\u001b[0m     rechunk\u001b[39m=\u001b[39;49mrechunk,\n\u001b[1;32m    929\u001b[0m     skip_rows_after_header\u001b[39m=\u001b[39;49mskip_rows_after_header,\n\u001b[1;32m    930\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    931\u001b[0m     row_count_name\u001b[39m=\u001b[39;49mrow_count_name,\n\u001b[1;32m    932\u001b[0m     row_count_offset\u001b[39m=\u001b[39;49mrow_count_offset,\n\u001b[1;32m    933\u001b[0m     try_parse_dates\u001b[39m=\u001b[39;49mtry_parse_dates,\n\u001b[1;32m    934\u001b[0m     eol_char\u001b[39m=\u001b[39;49meol_char,\n\u001b[1;32m    935\u001b[0m     raise_if_empty\u001b[39m=\u001b[39;49mraise_if_empty,\n\u001b[1;32m    936\u001b[0m     truncate_ragged_lines\u001b[39m=\u001b[39;49mtruncate_ragged_lines,\n\u001b[1;32m    937\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.10/site-packages/polars/lazyframe/frame.py:373\u001b[0m, in \u001b[0;36mLazyFrame._scan_csv\u001b[0;34m(cls, source, has_header, separator, comment_char, quote_char, skip_rows, dtypes, schema, null_values, missing_utf8_is_empty_string, ignore_errors, cache, with_column_names, infer_schema_length, n_rows, encoding, low_memory, rechunk, skip_rows_after_header, row_count_name, row_count_offset, try_parse_dates, eol_char, raise_if_empty, truncate_ragged_lines)\u001b[0m\n\u001b[1;32m    370\u001b[0m     sources \u001b[39m=\u001b[39m []  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m)\n\u001b[0;32m--> 373\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ldf \u001b[39m=\u001b[39m PyLazyFrame\u001b[39m.\u001b[39;49mnew_from_csv(\n\u001b[1;32m    374\u001b[0m     source,\n\u001b[1;32m    375\u001b[0m     sources,\n\u001b[1;32m    376\u001b[0m     separator,\n\u001b[1;32m    377\u001b[0m     has_header,\n\u001b[1;32m    378\u001b[0m     ignore_errors,\n\u001b[1;32m    379\u001b[0m     skip_rows,\n\u001b[1;32m    380\u001b[0m     n_rows,\n\u001b[1;32m    381\u001b[0m     cache,\n\u001b[1;32m    382\u001b[0m     dtype_list,\n\u001b[1;32m    383\u001b[0m     low_memory,\n\u001b[1;32m    384\u001b[0m     comment_char,\n\u001b[1;32m    385\u001b[0m     quote_char,\n\u001b[1;32m    386\u001b[0m     processed_null_values,\n\u001b[1;32m    387\u001b[0m     missing_utf8_is_empty_string,\n\u001b[1;32m    388\u001b[0m     infer_schema_length,\n\u001b[1;32m    389\u001b[0m     with_column_names,\n\u001b[1;32m    390\u001b[0m     rechunk,\n\u001b[1;32m    391\u001b[0m     skip_rows_after_header,\n\u001b[1;32m    392\u001b[0m     encoding,\n\u001b[1;32m    393\u001b[0m     _prepare_row_count_args(row_count_name, row_count_offset),\n\u001b[1;32m    394\u001b[0m     try_parse_dates,\n\u001b[1;32m    395\u001b[0m     eol_char\u001b[39m=\u001b[39;49meol_char,\n\u001b[1;32m    396\u001b[0m     raise_if_empty\u001b[39m=\u001b[39;49mraise_if_empty,\n\u001b[1;32m    397\u001b[0m     truncate_ragged_lines\u001b[39m=\u001b[39;49mtruncate_ragged_lines,\n\u001b[1;32m    398\u001b[0m     schema\u001b[39m=\u001b[39;49mschema,\n\u001b[1;32m    399\u001b[0m )\n\u001b[1;32m    400\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mComputeError\u001b[0m: error open file: data/test_events.csv, error open file: data/test_events.csv, No such file or directory (os error 2)"
     ]
    }
   ],
   "source": [
    "df_events = pl.scan_csv(\"data/test_events.csv\").with_columns(\n",
    "    timestamp + [pl.col(\"step\").cast(pl.UInt32)]\n",
    ").drop_nulls().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mismatch Onset and Wakeup are : \n",
      " shape: (5, 2)\n",
      "┌──────────────┬───────┐\n",
      "│ series_id    ┆ night │\n",
      "│ ---          ┆ ---   │\n",
      "│ str          ┆ i64   │\n",
      "╞══════════════╪═══════╡\n",
      "│ 0ce74d6d2106 ┆ 20    │\n",
      "│ 154fe824ed87 ┆ 30    │\n",
      "│ 44a41bba1ee7 ┆ 10    │\n",
      "│ efbfc4526d58 ┆ 7     │\n",
      "│ f8a8da8bdd00 ┆ 17    │\n",
      "└──────────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "# Removing null events and nights with mismatched counts from series_events\n",
    "mismatches = df_events.group_by(['series_id', 'night']).agg(\n",
    "    (pl.col('event') == 'onset').sum().alias('onset'),\n",
    "    (pl.col('event') == 'wakeup').sum().alias('wakeup')\n",
    "    ).sort(by=['series_id', 'night']).filter(pl.col('onset') != pl.col('wakeup')).select(pl.all().exclude('onset', 'wakeup'))\n",
    "print(f\"The mismatch Onset and Wakeup are : \\n {mismatches}\")\n",
    "df_events = df_events.join(mismatches, on=['series_id', 'night'], how='anti')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count for each series_id the number of onset and wakeup events\n",
    "df_events_problem = df_events.group_by(['series_id']).agg(\n",
    "    (pl.col('event') == 'onset').sum().alias('onset'),\n",
    "    (pl.col('event') == 'wakeup').sum().alias('wakeup')\n",
    "    ).sort(by=['series_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mismatch Onset and Wakeup are : \n",
      " shape: (0, 1)\n",
      "┌───────────┐\n",
      "│ series_id │\n",
      "│ ---       │\n",
      "│ str       │\n",
      "╞═══════════╡\n",
      "└───────────┘\n"
     ]
    }
   ],
   "source": [
    "# display the series_id with mismatched counts\n",
    "mismatches = df_events_problem.filter(pl.col('onset') != pl.col('wakeup')).select(pl.all().exclude('onset', 'wakeup'))\n",
    "print(f\"The mismatch Onset and Wakeup are : \\n {mismatches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_signals.join_asof(\n",
    "        df_events.drop('timestamp'),\n",
    "        on='step',\n",
    "        by='series_id',\n",
    "        strategy='backward',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Annotation Sleep // Awake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(\n",
    "        state= pl.when((pl.col('event')=='onset')).then(1).otherwise(0),\n",
    "    ).select(\n",
    "        pl.all().exclude('event','night')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df.with_columns(\n",
    "        delta = pl.col('state').shift(-1) - pl.col('state'),\n",
    "    ).with_columns(\n",
    "        wakeup = pl.when(pl.col('delta') == -1).then(True).otherwise(False),\n",
    "        onset = pl.when(pl.col('delta') == 1).then(True).otherwise(False),\n",
    "    )\n",
    ").drop('delta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort(by=['series_id', 'timestamp'])\n",
    "df = df.drop('step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Missing Data**\n",
    "\n",
    "Remove signals 6 hours after awake and 6 hours before sleep when an annotation is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each parquet file representing a time series\n",
    "# We will sort them by timestamp\n",
    "# if there are periods with 20 hours without sleep\n",
    "# We will remove a period of 16 hours because we consider the annotations as missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_train_dataset(train_data, train_events, drop_nulls=False) :\n",
    "    \n",
    "#     series_ids = train_data['series_id'].unique(maintain_order=True).to_list()\n",
    "#     X, y = pl.DataFrame(), pl.DataFrame()\n",
    "#     for idx in tqdm(series_ids) : \n",
    "        \n",
    "#         # Normalizing sample features\n",
    "#         sample = train_data.filter(pl.col('series_id')==idx).with_columns(\n",
    "#             [(pl.col(col) / pl.col(col).std()).cast(pl.Float32) for col in feature_cols if col != 'hour']\n",
    "#         )\n",
    "        \n",
    "#         events = train_events.filter(pl.col('series_id')==idx)\n",
    "        \n",
    "#         if drop_nulls : \n",
    "#             # Removing datapoints on dates where no data was recorded\n",
    "#             sample = sample.filter(\n",
    "#                 pl.col('timestamp').dt.date().is_in(events['timestamp'].dt.date())\n",
    "#             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Smoothing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (277, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>series_id</th><th>count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;78569a801a38&quot;</td><td>1433880</td></tr><tr><td>&quot;f564985ab692&quot;</td><td>1052820</td></tr><tr><td>&quot;fb223ed2278c&quot;</td><td>918360</td></tr><tr><td>&quot;f56824b503a0&quot;</td><td>846360</td></tr><tr><td>&quot;cfeb11428dd7&quot;</td><td>809820</td></tr><tr><td>&quot;062dbd4c95e6&quot;</td><td>778680</td></tr><tr><td>&quot;f0482490923c&quot;</td><td>761940</td></tr><tr><td>&quot;6ca4f4fca6a2&quot;</td><td>759240</td></tr><tr><td>&quot;d043c0ca71cd&quot;</td><td>745020</td></tr><tr><td>&quot;12d01911d509&quot;</td><td>744480</td></tr><tr><td>&quot;c107b5789660&quot;</td><td>740880</td></tr><tr><td>&quot;ebb6fae8ed43&quot;</td><td>737820</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;d515236bdeec&quot;</td><td>235620</td></tr><tr><td>&quot;83fa182bec3a&quot;</td><td>203760</td></tr><tr><td>&quot;99b829cbad2d&quot;</td><td>200880</td></tr><tr><td>&quot;9aed9ee12ae2&quot;</td><td>195840</td></tr><tr><td>&quot;a88088855de5&quot;</td><td>192780</td></tr><tr><td>&quot;a9e5f5314bcb&quot;</td><td>155160</td></tr><tr><td>&quot;5e816f11f5c3&quot;</td><td>136620</td></tr><tr><td>&quot;c535634d7dcd&quot;</td><td>136080</td></tr><tr><td>&quot;1c7c0bad1263&quot;</td><td>115380</td></tr><tr><td>&quot;60e51cad2ffb&quot;</td><td>113940</td></tr><tr><td>&quot;3a9a9dc2cbd9&quot;</td><td>103500</td></tr><tr><td>&quot;349c5562ee2c&quot;</td><td>37080</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (277, 2)\n",
       "┌──────────────┬─────────┐\n",
       "│ series_id    ┆ count   │\n",
       "│ ---          ┆ ---     │\n",
       "│ str          ┆ u32     │\n",
       "╞══════════════╪═════════╡\n",
       "│ 78569a801a38 ┆ 1433880 │\n",
       "│ f564985ab692 ┆ 1052820 │\n",
       "│ fb223ed2278c ┆ 918360  │\n",
       "│ f56824b503a0 ┆ 846360  │\n",
       "│ …            ┆ …       │\n",
       "│ 1c7c0bad1263 ┆ 115380  │\n",
       "│ 60e51cad2ffb ┆ 113940  │\n",
       "│ 3a9a9dc2cbd9 ┆ 103500  │\n",
       "│ 349c5562ee2c ┆ 37080   │\n",
       "└──────────────┴─────────┘"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.group_by('series_id').agg(pl.count().alias('count')).sort(by='count', descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write_parquet('data/test_dataset.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wow = df.filter(\n",
    "    pl.col('series_id') == 'f564985ab692'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wow.write_parquet('data/f564985ab692.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stratified Export**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
